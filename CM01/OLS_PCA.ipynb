{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: OLS Regression with numpy\n",
    "\n",
    "<p>In this exercise, we will compute the parameters (weights) of an OLS regression using only numpy (and our linear algebra knowledge).</p>\n",
    "<p>The forward equation for OLS regression is given by:</p>\n",
    "\n",
    "$$\\hat{y} = \\mathbf{X}\\beta$$\n",
    "\n",
    "where $\\mathbf{X}$ is an $N{\\times}M$ matrix of covariates, and $\\beta$ is a vector of parameters estimated from the data. In OLS regression, we seek to minimize the MSE (mean squared error) criterion:\n",
    "\n",
    "$$MSE(\\beta) = \\frac{1}N\\sum_{i=1}^N(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "<p>which can be written in matrix form as</p>\n",
    "\n",
    "$$MSE(\\beta) = (y - \\mathbf{X}\\beta)^T(y - \\mathbf{X}\\beta)$$\n",
    "\n",
    "setting the gradient of the MSE criterion to zero and solving for $y$, we obtain the following solution which minimizes the MSE:<br>\n",
    "\n",
    "$$\\beta^* = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$$\n",
    "\n",
    "See this excellent post for a full derivation: https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression\n",
    "\n",
    "Thus, in order to complete this exercise, you need to follow these steps:\n",
    "1. Generate X (covariates) and y (outcome) by calling the `simulate_ols_data()` function\n",
    "2. Complete the `ols(X, y)` function by computing the solution to the normal equation\n",
    "3. Compare your best parameters with the parameters returned by the numpy function `numpy.linalg.lstsq(X, y)`\n",
    "4. Compute RMSE on the training data by completing the function `rmse(y, y_hat)`. The RMSE is given by:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$RMSE= \\sqrt{\\frac{1}N\\sum_{i=1}^N(y_i - \\hat{y_i})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ols_data(N=100, M=3):\n",
    "    \"\"\"Function to simulate synthetic regression data.\"\"\"\n",
    "    \n",
    "    # Generate design matrix\n",
    "    X = np.random.randn(N, M)\n",
    "\n",
    "    # Add intercept to model\n",
    "    X = np.c_[np.ones(shape=(N, 1)), X]\n",
    "\n",
    "    # Generate true beta ~ N(0, 1)\n",
    "    beta_true = np.random.randn(M + 1)\n",
    "\n",
    "    # Compute noisy linear function\n",
    "    y = X @ beta_true + np.random.randn(N)\n",
    "\n",
    "    return X, y, beta_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinary_least_squares(X, y):\n",
    "    \"\"\"Function to compute the parameters of an OLS regression.\"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, y_hat):\n",
    "    \"\"\"Function to compute the RMSE between predictions and outcomes.\"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Simulate regression data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X, y, beta_true \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_ols_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 2. Obtain best parameters\u001b[39;00m\n\u001b[1;32m      5\u001b[0m beta_hat \u001b[38;5;241m=\u001b[39m ordinary_least_squares(X, y)\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36msimulate_ols_data\u001b[0;34m(N, M)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Function to simulate synthetic regression data.\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Generate design matrix\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(N, M)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Add intercept to model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mc_[np\u001b[38;5;241m.\u001b[39mones(shape\u001b[38;5;241m=\u001b[39m(N, \u001b[38;5;241m1\u001b[39m)), X]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Simulate regression data\n",
    "X, y, beta_true = simulate_ols_data(N=10000)\n",
    "\n",
    "# 2. Obtain best parameters\n",
    "beta_hat = ordinary_least_squares(X, y)\n",
    "\n",
    "# 3. Obtain best parameters using numpy.linalg.lstsq\n",
    "beta_hat_np = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "# 4. Compute RMSE on training data\n",
    "y_hat = X @ beta_hat_np\n",
    "\n",
    "rmse_train = rmse(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True betas: ', beta_true)\n",
    "print('OLS betas: ', beta_hat)\n",
    "print('Numpy LSTSQ betas', beta_hat_np)\n",
    "print('RMSE train: ', rmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final exercise: PCA\n",
    "\n",
    "![PCA meme](https://media.makeameme.org/created/brace-yourself-pca.jpg)\n",
    "\n",
    "<p>PCA learns a representation of multidimensional data which has lower dimensionality than the original data.</p>\n",
    "Furthermore is an orthogonal linear projection of the data, meaning that it decorrelates the dimensions of the learned representation.\n",
    "\n",
    "Consider a data matrix $\\mathbf{X}$. Suppose further that the data has been standardized (centered and scaled). The unbiased estimate of the covariance matrix is given by:\n",
    "\n",
    "$$\\widehat{Cov}[\\mathbf{X}] = \\frac{1}{m - 1}\\mathbf{X}^T\\mathbf{X}$$\n",
    "\n",
    "A popular variant of PCA performs an eigendecomposition of $\\mathbf{X}^T\\mathbf{X}$ defined by:\n",
    "\n",
    "$$\\widehat{Cov}[\\mathbf{X}] = \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^T$$\n",
    "\n",
    "where $\\mathbf{V}$ is the matrix of <em>eigenvectors</em> of $\\widehat{Cov}[\\mathbf{X}]$ and $\\mathbf{\\Lambda}$ is a diagonal matrix of <em>eigenvalues</em>.\n",
    "\n",
    "(Note, that sometimes the decomposition is performed on the scatter matrix instead $\\mathbf{X}^T\\mathbf{X}$. This does not change the eigenvectors, but simply scales the eigenvalues.) \n",
    "\n",
    "In other words, we decompose the covariance matrix into a scale part (eigenvalues) and direction part (eigenvectors). The eigenvectors are the principle components of the $\\widehat{Cov}[\\mathbf{X}]$.\n",
    "\n",
    "Eigenvectors determine orthogonal directions in feature space. Each component of the eigenvector encodes how each of the original dimension directions contributes to the eigenvector. The first eigenvector (first PC) encodes the direction of most variance, and so on.\n",
    "Eigenvalues represent how much of the original variance is captured (explained) in the direction of the corresponding eigenvector.\n",
    "\n",
    "Read in the data `extraversion_big5.csv` and complete the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/extraversion_big5.csv', sep=';', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "### Your code here\n",
    "\n",
    "# Compute covariance matrix\n",
    "cov = None\n",
    "\n",
    "# Perform an eigendecomposition\n",
    "### Your code here\n",
    "lambd, V = None, None\n",
    "\n",
    "# Check reconstruction of the covariance matrix\n",
    "np.allclose(V @ np.diag(lambd) @ V.T, cov) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results\n",
    "f, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(np.arange(1, lambd.shape[0]+1), lambd / np.sum(lambd), '-o', color='#aa0000')\n",
    "ax.set_xlim([1 - 0.1, 10 + 0.1])\n",
    "ax.set_xlabel('Eigenvalue index')\n",
    "ax.set_ylabel('Proportion of explained variance')\n",
    "ax.set_title('Eigenvalues plot')\n",
    "ax.grid(alpha=0.3)\n",
    "sns.despine(ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
